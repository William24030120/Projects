{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "K9nyiFF3wBEH"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# FCFG_generator\n",
        "Sentence generator for Context Free Grammars with features.\n",
        "This is based on the nltk Feature Context Free Grammars. However, that system doesn't generate sentences correctly. It seems that the feature constraints only work for parsing, not generating. Possibly this is just a bug, but it seems like the nltk infrastructure may not support the kind of unification that is required. So I've writting that from scratch. So I only use the grammar parser and FCFC data structure from nltk. But that should mean that this generator will work with any FCFG created with nltk.\n"
      ],
      "metadata": {
        "id": "qLf30CDkbNBK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code for FCFGs and Feature Unification\n",
        "You don't need to understand the implmentation to create FCRG grammars and use the FCFG sentence generator."
      ],
      "metadata": {
        "id": "K9nyiFF3wBEH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "80BHi8JYbIvv"
      },
      "outputs": [],
      "source": [
        "import nltk, copy, random, copy\n",
        "\n",
        "# Go through all feature values of all nodes in a list of nodes.\n",
        "# Return a mapping from distinct variable names to 'slots'.\n",
        "# The slots are empty lists into which values can later be inserted.\n",
        "def get_nodelist_variable_map(nodelist):\n",
        "    varmap = {}\n",
        "    for node in nodelist:\n",
        "      if type(node)==str: continue\n",
        "      for feature in node:\n",
        "        if type(node[feature]) == nltk.sem.logic.Variable:\n",
        "          varmap[node[feature].name] = []\n",
        "    return varmap\n",
        "\n",
        "## Copy nodelist and replace all variables with slots\n",
        "## Instances of the same variable will get the same slot value.\n",
        "def nodelist_replace_variables(nodelist):\n",
        "    nodelist = copy.deepcopy(nodelist)\n",
        "    varmap = get_nodelist_variable_map(nodelist)\n",
        "    for node in nodelist:\n",
        "      if type(node)==str: continue\n",
        "      for feature in node:\n",
        "          if type(node[feature]) == nltk.sem.logic.Variable:\n",
        "              node[feature] = varmap[node[feature].name]\n",
        "    return nodelist\n",
        "\n",
        "## Copy a node, replacing Variable feature values with 'slots'\n",
        "def replace_node_vars(node):\n",
        "  if type(node)==str: return node  # quick return for terminals\n",
        "  ## Otherwise just do it as a nodelist with one element\n",
        "  return nodelist_replace_variables([node])[0]\n",
        "\n",
        "## Copy a production rule, replacing Variable feature values with 'slots'\n",
        "def replace_production_vars(prod):\n",
        "  ## Turn the rule into a node list\n",
        "  nodes = [prod.lhs()] + list(prod.rhs())\n",
        "  nodes = nodelist_replace_variables(nodes)\n",
        "  ## Turn the result back into a production rule\n",
        "  return nltk.Production(nodes[0], nodes[1:])\n",
        "\n",
        "## Unpack nested slots untill you get either empty list (variable slot)\n",
        "## Or you get a value (the slot variable has been instantiated)\n",
        "## If v is not a slot you just get back v.\n",
        "def unslot(v):\n",
        "  while type(v)== list:\n",
        "    if v == []: return v # return the slot\n",
        "    v = v[0]             # unpack list\n",
        "  return v               # return any non-list value\n",
        "\n",
        "def unify_values(v1, v2):\n",
        "      v1 = unslot(v1)\n",
        "      v2 = unslot(v2)\n",
        "      # Case of both str values\n",
        "      if type(v1)==str and type(v2)==str:\n",
        "          return v1==v2\n",
        "      # Cases of one str and one slot (variable) feature value\n",
        "      # Insert the str into the slot.\n",
        "      if type(v1)==str:\n",
        "            v2.insert(0,v1)\n",
        "      elif type(v2)==str:\n",
        "            v1.insert(0,v2)\n",
        "      else:\n",
        "          # Remaining case: both are slots\n",
        "          slot = []   # Bind by adding new slot within each of them:\n",
        "          v1.insert(0, slot)\n",
        "          v2.insert(0, slot)\n",
        "      return True\n",
        "\n",
        "def unify_nodes( n1, n2 ):\n",
        "    for feature in n1:\n",
        "        if feature in n2:\n",
        "            if not unify_values( n1[feature], n2[feature] ):\n",
        "                return False\n",
        "    return True\n",
        "\n",
        "def node_type(node):\n",
        "  return list(node.items())[0][1] # is there an easier way??\n",
        "\n",
        "\n",
        "## The main generator function\n",
        "def generate_random_from_grammar(grammar, node=None, depth=0, debug=False):\n",
        "    def dbprint(*args):\n",
        "      if debug: print(*args)\n",
        "\n",
        "    if node == None:\n",
        "      node = grammar.start()\n",
        "      node = replace_node_vars(node)\n",
        "    dbprint(\"***DEPTH:\", depth)\n",
        "    dbprint(\"* Generating from node:\\n\", node)\n",
        "    dbprint(\"--- end of node ---\") \n",
        "    rule_options = grammar.productions(lhs=node)\n",
        "    if debug:\n",
        "      print(\"Rule options:\")\n",
        "      for r in rule_options:\n",
        "          print(r)\n",
        "    random.shuffle(rule_options) # First shuffle then pick first match (for speed)\n",
        "    chosen_rule = None\n",
        "    for rule in rule_options:\n",
        "      rule = replace_production_vars(rule) # gives a copy with var slots added\n",
        "      rule_head = rule.lhs()\n",
        "      # we test that a copy of the node will unify\n",
        "      # (Don't want to start binding the acual node in case it fails half way/)\n",
        "      node_copy = copy.deepcopy(node) \n",
        "      if unify_nodes( node_copy, rule_head ):  # It's a Match!\n",
        "        chosen_rule = rule\n",
        "        break\n",
        "    if not chosen_rule:\n",
        "      print(\"!!! No applicable rules were found to expand node:\\n\", node)\n",
        "      return False\n",
        "    dbprint(\"chosen_rule:\", chosen_rule)\n",
        "    ## NOW UNIFY THE CHOSEN RULE HEAD WITH THE NODE\n",
        "    unify_nodes( node, chosen_rule.lhs() )\n",
        "    ## Vars in the chosen rule will now be instantiated as required\n",
        "    ## (but this is just a copy of the actual rule in the grammar)\n",
        "    ## Now expand the children in the matched and instantiated rule copy:\n",
        "    #children = [node_type(node)]\n",
        "    children = []\n",
        "    for item in chosen_rule.rhs():\n",
        "      dbprint(\"Now instantiate item, type:\", item, type(item))\n",
        "      if type(item) == nltk.grammar.FeatStructNonterminal:\n",
        "        child = generate_random_from_grammar(grammar, node=item, depth=depth+1)\n",
        "        children.append(child)\n",
        "      else:\n",
        "        children.append(item)\n",
        "    ## Return structure as list of node type and children\n",
        "    return [node_type(node)] + children\n",
        "\n",
        "# Now using unpack istead of unpack because I've added node types\n",
        "# to the structure and need to throw them away.    \n",
        "def flatten(L):\n",
        "    if L == []: return L\n",
        "    if type(L[0]) == list:\n",
        "        return flatten(L[0]) + flatten(L[1:])\n",
        "    return [L[0]] + flatten(L[1:])\n",
        "\n",
        "def unpack(struct):\n",
        "  if struct == [] : return []\n",
        "  if type(struct) == list:\n",
        "    result = []\n",
        "    for x in struct[1:]:\n",
        "      result.extend(unpack(x))\n",
        "    return result\n",
        "  return [struct]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def fcfg_from_string( string ):\n",
        "    return nltk.grammar.FeatureGrammar.fromstring(string)\n",
        "\n",
        "def make_node( ntype, **kwargs ):\n",
        "    return nltk.grammar.FeatStructNonterminal( ntype, **kwargs )\n",
        "\n",
        "def show_random_sentence(grammar, \n",
        "                         node=None, # start node\n",
        "                         n=1,       # number of sentences\n",
        "                         show_sentence=True, show_structure=True, debug=False):\n",
        "    if type(grammar) == str:\n",
        "      grammar = fcfg_from_string( grammar )\n",
        "    for _ in range(n):\n",
        "      structure = generate_random_from_grammar(grammar, node=node, debug=debug)\n",
        "      if show_structure: \n",
        "        print(structure)\n",
        "      if show_sentence:\n",
        "        #words = flatten(structure)\n",
        "        words = unpack(structure)\n",
        "        words = [(word if word else \"!NO_EXPANSION!\") for word in words]\n",
        "        words[0] = words[0].title()\n",
        "        sentence = \" \".join(words) + \".\"\n",
        "        print(sentence)"
      ],
      "metadata": {
        "id": "hHqplFHK5ViQ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example Grammars\n",
        "The `EG_GRAMMAR` string below illustrates FCFG grammar specification. You can devise whatever grammar you fancy within the general framework of Context Free Grammar rules augmented with _features_ and _feature variables_."
      ],
      "metadata": {
        "id": "v5OcmRf0u6OY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EG_GRAMMAR = \"\"\"\n",
        "% start S\n",
        "S[ATYPE=?at] -> NP[NUM=?n,ATYPE=?at] VP[NUM=?n,ATYPE=?at]\n",
        "\n",
        "NP[NUM=sg,ATYPE=?at] -> PN[NUM=sg,ATYPE=?at]\n",
        "NP[NUM=?n,ATYPE=?at] -> Det[NUM=?n] CN[NUM=?n,ATYPE=?at]\n",
        "\n",
        "PN[NUM=sg,ATYPE=intelligent] -> 'Bob' | 'John' | 'Alice' | 'Mary' | 'Karen'\n",
        "\n",
        "Det -> 'the' | 'some'\n",
        "Det[NUM=sg] -> 'a' | 'one'\n",
        "Det[NUM=pl] -> 'several' | 'two'\n",
        "\n",
        "CN[NUM=sg,ATYPE='animate'] -> 'dog'  | 'cat' | 'horse'\n",
        "CN[NUM=pl,ATYPE='animate'] -> 'dogs' | 'cats' |'horses'\n",
        "CN[NUM=sg,ATYPE='intelligent'] -> 'student' \n",
        "CN[NUM=pl,ATYPE='intelligent'] -> 'students'\n",
        "\n",
        "VP[NUM=sg,ATYPE='intelligent'] -> 'sings' | 'laughs'\n",
        "VP[NUM=pl,ATYPE='intelligent'] -> 'sing'  | 'laugh'\n",
        "VP[NUM=sg,ATYPE='animate'] -> 'runs' | 'jumps'\n",
        "VP[NUM=pl,ATYPE='animate'] -> 'run'  | 'jump'\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "vqEuBi8Ub8Yv"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can generate some random sentences based on the grammar."
      ],
      "metadata": {
        "id": "dNMqeVM96urw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "show_random_sentence(EG_GRAMMAR, n=10, node=None, show_structure=False, debug=False)"
      ],
      "metadata": {
        "id": "ENrJIXyn6teU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c2246af-1d10-4bac-f2c5-68771f7aaa63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Alice laughs.\n",
            "Mary laughs.\n",
            "John laughs.\n",
            "Mary sings.\n",
            "The student laughs.\n",
            "Several cats run.\n",
            "Some dogs run.\n",
            "Karen sings.\n",
            "Two horses jump.\n",
            "Two dogs jump.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can set `show_structure=True` to see the phrase structure of the sentences."
      ],
      "metadata": {
        "id": "PdrZ7Qr87AfA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "show_random_sentence(EG_GRAMMAR, n=5, node=None, show_structure=True)"
      ],
      "metadata": {
        "id": "99FN202hyyNg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0e07bb0-37ce-4a7e-aedb-d646b568ad20"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['S', ['NP', ['Det', 'several'], ['CN', 'horses']], ['VP', 'run']]\n",
            "Several horses run.\n",
            "['S', ['NP', ['Det', 'several'], ['CN', 'students']], ['VP', 'laugh']]\n",
            "Several students laugh.\n",
            "['S', ['NP', ['PN', 'Alice']], ['VP', 'sings']]\n",
            "Alice sings.\n",
            "['S', ['NP', ['PN', 'John']], ['VP', 'sings']]\n",
            "John sings.\n",
            "['S', ['NP', ['Det', 'some'], ['CN', 'cat']], ['VP', 'runs']]\n",
            "Some cat runs.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can make a start node with some features set (as if keyword args). The Default is to start at S with no features set. So if we want to generate sentences involving intelligent actions we could create the following:"
      ],
      "metadata": {
        "id": "APiApFuuw22g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "s_intelligent = make_node('S', ATYPE='intelligent')"
      ],
      "metadata": {
        "id": "j9J53XNNeU7Z"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then you can generate sentences (or other expressions) starting with that type of node:"
      ],
      "metadata": {
        "id": "wVubl-kLxJqY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "show_random_sentence(EG_GRAMMAR, n=5, node=s_intelligent, show_structure=False, debug=False)"
      ],
      "metadata": {
        "id": "j4bhhZV8xC7i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a238e93f-daf0-409a-9d66-5dc3da5d255d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Several students sing.\n",
            "One student laughs.\n",
            "Karen laughs.\n",
            "One student laughs.\n",
            "Bob laughs.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Customised Coarse-Grained Grammars\n",
        "You don't necessarily need to use the GFSGs framework to specify the detailed syntactic structure of natual language. They are also useful if we want to specify classes of example sentences over a limited range of patterns that we may be interested in. For example, we could generate examples to be used for testing the performance of Language Models."
      ],
      "metadata": {
        "id": "8tQRtRW575DJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "POURING_GRAMMAR = \"\"\"\n",
        "S -> LEFT CON RIGHT\n",
        "CON -> 'because'\n",
        "LEFT -> 'The' PERSON_ADJ PERSON 'poured the' WINE_ADJ 'wine into the cup'\n",
        "PERSON_ADJ -> 'happy' | 'sad' | 'unlucky'\n",
        "PERSON -> 'cook' | 'student'\n",
        "RIGHT -> 'they were' REASON_ADJ\n",
        "WINE_ADJ -> 'red' | 'white'\n",
        "REASON_ADJ -> 'thirsty' | 'in need of some relaxation'\n",
        "\"\"\"\n",
        "\n",
        "show_random_sentence(POURING_GRAMMAR, n=20, node=None, show_structure=False, debug=False)"
      ],
      "metadata": {
        "id": "F0FUXOb5b_85",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6c8a908-50dd-479c-a95e-38cc4e244760"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The unlucky cook poured the white wine into the cup because they were thirsty.\n",
            "The unlucky student poured the red wine into the cup because they were in need of some relaxation.\n",
            "The happy student poured the white wine into the cup because they were thirsty.\n",
            "The sad cook poured the red wine into the cup because they were thirsty.\n",
            "The happy student poured the red wine into the cup because they were thirsty.\n",
            "The unlucky student poured the white wine into the cup because they were in need of some relaxation.\n",
            "The happy cook poured the red wine into the cup because they were thirsty.\n",
            "The happy cook poured the red wine into the cup because they were in need of some relaxation.\n",
            "The happy student poured the red wine into the cup because they were thirsty.\n",
            "The happy student poured the red wine into the cup because they were thirsty.\n",
            "The happy student poured the white wine into the cup because they were in need of some relaxation.\n",
            "The sad student poured the red wine into the cup because they were in need of some relaxation.\n",
            "The happy cook poured the red wine into the cup because they were in need of some relaxation.\n",
            "The happy cook poured the white wine into the cup because they were thirsty.\n",
            "The unlucky student poured the red wine into the cup because they were thirsty.\n",
            "The unlucky student poured the white wine into the cup because they were in need of some relaxation.\n",
            "The unlucky cook poured the red wine into the cup because they were in need of some relaxation.\n",
            "The unlucky cook poured the white wine into the cup because they were thirsty.\n",
            "The sad cook poured the red wine into the cup because they were thirsty.\n",
            "The unlucky cook poured the red wine into the cup because they were thirsty.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test"
      ],
      "metadata": {
        "id": "2SffuXjdMSNW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}